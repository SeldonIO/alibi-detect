{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[source](../api/alibi_detect.cd.mmd.rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Mean Discrepancy\n",
    "\n",
    "## Overview\n",
    "\n",
    "The [Maximum Mean Discrepancy (MMD)](http://jmlr.csail.mit.edu/papers/v13/gretton12a.html) detector is a kernel-based method for multivariate 2 sample testing. The MMD is a distance-based measure between 2 distributions *p* and *q* based on the mean embeddings $\\mu_{p}$ and $\\mu_{q}$ in a reproducing kernel Hilbert space $F$:\n",
    "\n",
    "\\begin{align}\n",
    "MMD(F, p, q) & = || \\mu_{p} - \\mu_{q} ||^2_{F} \\\\\n",
    "\\end{align}\n",
    "\n",
    "We can compute unbiased estimates of $MMD^2$ from the samples of the 2 distributions after applying the kernel trick. We use by default a [radial basis function kernel](https://en.wikipedia.org/wiki/Radial_basis_function_kernel), but users are free to pass their own kernel of preference to the detector. We obtain a $p$-value via a [permutation test](https://en.wikipedia.org/wiki/Resampling_(statistics)) on the values of $MMD^2$.\n",
    "\n",
    "For high-dimensional data, we typically want to reduce the dimensionality before computing the permutation test. Following suggestions in [Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift](https://arxiv.org/abs/1810.11953), we incorporate Untrained AutoEncoders (UAE), black-box shift detection using the classifier's softmax outputs ([BBSDs](https://arxiv.org/abs/1802.03916)) and [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) as out-of-the box preprocessing methods. Preprocessing methods which do not rely on the classifier will usually pick up drift in the input data, while BBSDs focuses on label shift.\n",
    "\n",
    "Detecting input data drift (covariate shift) $\\Delta p(x)$ for text data requires a custom preprocessing step. We can pick up changes in the semantics of the input by extracting (contextual) embeddings and detect drift on those. Strictly speaking we are not detecting $\\Delta p(x)$ anymore since the whole training procedure (objective function, training data etc) for the (pre)trained embeddings has an impact on the embeddings we extract. The library contains functionality to leverage pre-trained embeddings from [HuggingFace's transformer package](https://github.com/huggingface/transformers) but also allows you to easily use your own embeddings of choice. Both options are illustrated with examples in the [Text drift detection on IMDB movie reviews](../examples/cd_text_imdb.nblink) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "### Initialize\n",
    "\n",
    "\n",
    "Parameters:\n",
    "\n",
    "* `p_val`: p-value used for significance of the permutation test.\n",
    "\n",
    "* `X_ref`: Data used as reference distribution.\n",
    "\n",
    "* `preprocess_X_ref`: Whether to already apply the (optional) preprocessing step to the reference data at initialization and store the preprocessed data. Dependent on the preprocessing step, this can reduce the computation time for the predict step significantly, especially when the reference dataset is large. Defaults to *True*.\n",
    "\n",
    "* `update_X_ref`: Reference data can optionally be updated to the last N instances seen by the detector or via [reservoir sampling](https://en.wikipedia.org/wiki/Reservoir_sampling) with size N. For the former, the parameter equals *{'last': N}* while for reservoir sampling *{'reservoir_sampling': N}* is passed.\n",
    "\n",
    "* `preprocess_fn`: Function to preprocess the data before computing the data drift metrics. Typically a dimensionality reduction technique.\n",
    "\n",
    "* `preprocess_kwargs`: Keyword arguments for `preprocess_fn`. Again see the notebooks for [image](../examples/cd_mmd_cifar10.nblink) and [text](../examples/cd_text_imdb.nblink) data for concrete, detailed examples. The built-in *UAE*, *BBSDs* or text-specific preprocessing steps are passed here as well. See below for a brief example.\n",
    "\n",
    "* `kernel`: Kernel function used when computing the MMD. Defaults to a Gaussian kernel.\n",
    "\n",
    "* `kernel_kwargs`: Keyword arguments for the kernel function. For the Gaussian kernel this is the kernel bandwidth `sigma`. We can also sum over a number of different kernel bandwidths. `sigma` then becomes an array with different values. If `sigma` is not specified, the detector will infer it by computing the pairwise distances between each of the instances in the 2 samples and set `sigma` to the median distance.\n",
    "\n",
    "* `n_permutations`: Number of permutations used in the permutation test.\n",
    "\n",
    "* `chunk_size`: Used to optionally compute the MMD between the 2 samples in chunks using [dask](https://dask.org/) to avoid potential out-of-memory errors. In terms of speed, the optimal chunk size is application and hardware dependent, so it is often worth to test a few different values, including *None*. *None* means that the computation is done in-memory in NumPy.\n",
    "\n",
    "* `data_type`: can specify data type added to metadata. E.g. *'tabular'* or *'image'*.\n",
    "\n",
    "Initialized drift detector example:\n",
    "\n",
    "```python\n",
    "from alibi_detect.cd import MMDDrift\n",
    "from alibi_detect.cd.preprocess import uae  # Untrained AutoEncoder\n",
    "\n",
    "encoder_net = tf.keras.Sequential(\n",
    "  [\n",
    "      InputLayer(input_shape=(32, 32, 3)),\n",
    "      Conv2D(64, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
    "      Conv2D(128, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
    "      Conv2D(512, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
    "      Flatten(),\n",
    "      Dense(32,)\n",
    "  ]\n",
    ")\n",
    "uae = UAE(encoder_net=encoder_net)\n",
    "\n",
    "cd = MMDDrift(\n",
    "    p_val=.05,\n",
    "    X_ref=X_ref,\n",
    "    preprocess_X_ref=True,\n",
    "    preprocess_kwargs={'model': uae, 'batch_size': 128},\n",
    "    kernel=gaussian_kernel,\n",
    "    kernel_kwargs={'sigma': np.array([.5, 1., 5.])},\n",
    "    chunk_size=1000,\n",
    "    n_permutations=1000\n",
    ")      \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Drift\n",
    "\n",
    "We detect data drift by simply calling `predict` on a batch of instances `X`. We can return the p-value of the permutation test by setting `return_p_val` to *True*.\n",
    "\n",
    "The prediction takes the form of a dictionary with `meta` and `data` keys. `meta` contains the detector's metadata while `data` is also a dictionary which contains the actual predictions stored in the following keys:\n",
    "\n",
    "* `is_drift`: 1 if the sample tested has drifted from the reference data and 0 otherwise.\n",
    "\n",
    "* `p_val`: contains the p-value if `return_p_val` equals *True*.\n",
    "\n",
    "\n",
    "```python\n",
    "preds_drift = cd.predict(X, return_p_val=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### Image\n",
    "\n",
    "[Drift detection on CIFAR10](../examples/cd_mmd_cifar10.nblink)\n",
    "\n",
    "[Text drift detection on IMDB movie reviews](../examples/cd_text_imdb.nblink)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:detect] *",
   "language": "python",
   "name": "conda-env-detect-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
