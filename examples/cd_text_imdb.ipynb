{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text drift detection on IMDB movie reviews\n",
    "\n",
    "## Method\n",
    "\n",
    "We detect drift on text data using both the [Maximum Mean Discrepancy](https://docs.seldon.io/projects/alibi-detect/en/latest/methods/mmddrift.html) and [Kolmogorov-Smirnov (K-S)](https://docs.seldon.io/projects/alibi-detect/en/latest/methods/ksdrift.html) detectors. In this example notebook we will focus on detecting covariate shift $\\Delta p(x)$ as detecting predicted label distribution drift does not differ from other modalities (check [K-S](https://docs.seldon.io/projects/alibi-detect/en/latest/examples/cd_ks_cifar10.html#BBSDs) and [MMD](https://docs.seldon.io/projects/alibi-detect/en/latest/examples/cd_mmd_cifar10.html#BBSDs) drift on CIFAR-10).\n",
    "\n",
    "It becomes however a little bit more involved when we want to pick up input data drift $\\Delta p(x)$. When we deal with tabular or image data, we can either directly apply the two sample hypothesis test on the input or do the test after a preprocessing step with for instance an Untrained AutoEncoder (*UAE*) as proposed in [Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift](https://arxiv.org/abs/1810.11953). It is not as straightforward when dealing with text, both in string or tokenized format as they don't directly represent the semantics of the input.\n",
    "\n",
    "As a result, we extract (contextual) embeddings for the text and detect drift on those. This procedure has a significant impact on the type of drift we detect. Strictly speaking we are not detecting $\\Delta p(x)$ anymore since the whole training procedure (objective function, training data etc) for the (pre)trained embeddings has an impact on the embeddings we extract.\n",
    "\n",
    "The library contains functionality to leverage pre-trained embeddings from [HuggingFace's transformer package](https://github.com/huggingface/transformers) but also allows you to easily use your own embeddings of choice. Both options are illustrated with examples in this notebook.\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "Binary sentiment classification [dataset](https://ai.stanford.edu/~amaas/data/sentiment/) containing $25,000$ movie reviews for training and $25,000$ for testing. We make use from the [nlp](https://github.com/huggingface/nlp) package for the data, which you can install via pip: `pip install nlp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel, AutoTokenizer, BertConfig\n",
    "from alibi_detect.cd import KSDrift, MMDDrift\n",
    "from alibi_detect.cd.preprocess import UAE\n",
    "from alibi_detect.models.embedding import TransformerEmbedding\n",
    "from alibi_detect.utils.saving import save_detector, load_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/avl/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/avl/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset: str, split: str = 'test'):\n",
    "    data = nlp.load_dataset(dataset)\n",
    "    X, y = [], []\n",
    "    for x in data[split]:\n",
    "        X.append(x['text'])\n",
    "        y.append(x['label'])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nlp.load:Checking /home/avl/.cache/huggingface/datasets/d3b7716978cb901261e59327d43b04c52d6d29e50eeac39bea0816865a584081.7c39fd6270c5ee55bcf2e4de23af77ef299e0df65be3f3e84454dcef7175844a.py for additional imports.\n",
      "INFO:filelock:Lock 140684143876368 acquired on /home/avl/.cache/huggingface/datasets/d3b7716978cb901261e59327d43b04c52d6d29e50eeac39bea0816865a584081.7c39fd6270c5ee55bcf2e4de23af77ef299e0df65be3f3e84454dcef7175844a.py.lock\n",
      "INFO:nlp.load:Found main folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/imdb/imdb.py at /home/avl/anaconda3/envs/detect/lib/python3.7/site-packages/nlp/datasets/imdb\n",
      "INFO:nlp.load:Found specific version folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/imdb/imdb.py at /home/avl/anaconda3/envs/detect/lib/python3.7/site-packages/nlp/datasets/imdb/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743\n",
      "INFO:nlp.load:Found script file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/imdb/imdb.py to /home/avl/anaconda3/envs/detect/lib/python3.7/site-packages/nlp/datasets/imdb/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743/imdb.py\n",
      "INFO:nlp.load:Updating dataset infos file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/imdb/dataset_infos.json to /home/avl/anaconda3/envs/detect/lib/python3.7/site-packages/nlp/datasets/imdb/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743/dataset_infos.json\n",
      "INFO:nlp.load:Found metadata file for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/imdb/imdb.py at /home/avl/anaconda3/envs/detect/lib/python3.7/site-packages/nlp/datasets/imdb/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743/imdb.json\n",
      "INFO:filelock:Lock 140684143876368 released on /home/avl/.cache/huggingface/datasets/d3b7716978cb901261e59327d43b04c52d6d29e50eeac39bea0816865a584081.7c39fd6270c5ee55bcf2e4de23af77ef299e0df65be3f3e84454dcef7175844a.py.lock\n",
      "INFO:nlp.builder:No config specified, defaulting to first: imdb/plain_text\n",
      "INFO:nlp.info:Loading Dataset Infos from /home/avl/anaconda3/envs/detect/lib/python3.7/site-packages/nlp/datasets/imdb/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743\n",
      "INFO:nlp.builder:Overwrite dataset info from restored data version.\n",
      "INFO:nlp.info:Loading Dataset info from /home/avl/.cache/huggingface/datasets/imdb/plain_text/1.0.0\n",
      "INFO:nlp.builder:Reusing dataset imdb (/home/avl/.cache/huggingface/datasets/imdb/plain_text/1.0.0)\n",
      "INFO:nlp.builder:Constructing Dataset for split None, from /home/avl/.cache/huggingface/datasets/imdb/plain_text/1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "X, y = load_dataset('imdb', split='train')\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at respectively a negative and positive review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n",
      "This is one of the dumbest films, I've ever seen. It rips off nearly ever type of thriller and manages to make a mess of them all.<br /><br />There's not a single good line or character in the whole mess. If there was a plot, it was an afterthought and as far as acting goes, there's nothing good to say so Ill say nothing. I honestly cant understand how this type of nonsense gets produced and actually released, does somebody somewhere not at some stage think, 'Oh my god this really is a load of shite' and call it a day. Its crap like this that has people downloading illegally, the trailer looks like a completely different film, at least if you have download it, you haven't wasted your time or money Don't waste your time, this is painful.\n"
     ]
    }
   ],
   "source": [
    "labels = ['Negative', 'Positive']\n",
    "print(labels[y[-1]])\n",
    "print(X[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love scenes in clothes warehouse are second to none. The corn on face is a classic, as good as anything in Blazing Saddles. The take on lawyers is also superb. After being accused of being a turncoat, selling out his boss, and being dishonest the lawyer of Pepto Bolt shrugs indifferently \"I'm a lawyer\" he says. Three funny words. Jeffrey Tambor, a favorite from the later Larry Sanders show, is fantastic here too as a mad millionaire who wants to crush the ghetto. His character is more malevolent than usual. The hospital scene, and the scene where the homeless invade a demolition site, are all-time classics. Look for the legs scene and the two big diggers fighting (one bleeds). This movie gets better each time I see it (which is quite often).\n"
     ]
    }
   ],
   "source": [
    "print(labels[y[2]])\n",
    "print(X[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the original test set in a reference dataset and a dataset which should not be rejected under the *H0* of the statistical test. We also create imbalanced datasets and inject selected words in the reference set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(X: np.ndarray, y: np.ndarray, proba_zero: float, n: int):\n",
    "    if len(y.shape) == 1:\n",
    "        idx_0 = np.where(y == 0)[0]\n",
    "        idx_1 = np.where(y == 1)[0]\n",
    "    else:\n",
    "        idx_0 = np.where(y[:, 0] == 1)[0]\n",
    "        idx_1 = np.where(y[:, 1] == 1)[0]\n",
    "    n_0, n_1 = int(n * proba_zero), int(n * (1 - proba_zero))\n",
    "    idx_0_out = np.random.choice(idx_0, n_0, replace=False)\n",
    "    idx_1_out = np.random.choice(idx_1, n_1, replace=False)\n",
    "    X_out = np.concatenate([X[idx_0_out], X[idx_1_out]])\n",
    "    y_out = np.concatenate([y[idx_0_out], y[idx_1_out]])\n",
    "    return X_out, y_out\n",
    "\n",
    "\n",
    "def padding_last(x: np.ndarray, seq_len: int) -> np.ndarray:\n",
    "    try:  # try not to replace padding token\n",
    "        last_token = np.where(x == 0)[0][0]\n",
    "    except:  # no padding\n",
    "        last_token = seq_len - 1\n",
    "    return 1, last_token\n",
    "\n",
    "\n",
    "def padding_first(x: np.ndarray, seq_len: int) -> np.ndarray:\n",
    "    try:  # try not to replace padding token\n",
    "        first_token = np.where(x == 0)[0][-1] + 2\n",
    "    except:  # no padding\n",
    "        first_token = 0\n",
    "    return first_token, seq_len - 1\n",
    "\n",
    "\n",
    "def inject_word(token: int, X: np.ndarray, perc_chg: float, padding: str = 'last'):\n",
    "    seq_len = X.shape[1]\n",
    "    n_chg = int(perc_chg * .01 * seq_len)\n",
    "    X_cp = X.copy()\n",
    "    for _ in range(X.shape[0]):\n",
    "        \n",
    "        if padding == 'last':\n",
    "            first_token, last_token = padding_last(X_cp[_, :], seq_len)\n",
    "        else:\n",
    "            first_token, last_token = padding_first(X_cp[_, :], seq_len)\n",
    "            \n",
    "        if last_token <= n_chg:\n",
    "            choice_len = seq_len\n",
    "        else:\n",
    "            choice_len = last_token\n",
    "            \n",
    "        idx = np.random.choice(np.arange(first_token, choice_len), n_chg, replace=False)\n",
    "        \n",
    "        X_cp[_, idx] = token\n",
    "    return X_cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference, *H0* and imbalanced data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frac_zero = fraction with label 0 (=negative sentiment)\n",
    "n_sample = 1000\n",
    "X_ref = random_sample(X, y, proba_zero=.5, n=n_sample)[0]\n",
    "X_h0 = random_sample(X, y, proba_zero=.5, n=n_sample)[0]\n",
    "n_imb = [.1, .9]\n",
    "X_imb = {_: random_sample(X, y, proba_zero=_, n=n_sample)[0] for _ in n_imb}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inject words in reference data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['fantastic', 'good', 'bad', 'horrible']\n",
    "perc_chg = [1., 5.]  # % of tokens to change in an instance\n",
    "\n",
    "words_tf = tokenizer.encode(words, return_tensors='tf')\n",
    "words_tf = list(words_tf.numpy()[0, 1:-1])\n",
    "max_len = 100\n",
    "tokens = tokenizer.batch_encode_plus(X_ref, pad_to_max_length=True, \n",
    "                                     max_length=max_len, return_tensors='tf')\n",
    "X_word = {}\n",
    "for i, w in enumerate(words_tf):\n",
    "    X_word[words[i]] = {}\n",
    "    for p in perc_chg:\n",
    "        x = inject_word(w, tokens['input_ids'].numpy(), p)\n",
    "        dec = tokenizer.batch_decode(x, **dict(skip_special_tokens=True))\n",
    "        X_word[words[i]][p] = np.array(dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "First we need to specify the type of embedding we want to extract from the BERT model. We can extract embeddings from the ...\n",
    "\n",
    "- **pooler_output**: Last layer hidden-state of the first token of the sequence (classification token; CLS) further processed by a Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence prediction (classification) objective during pre-training. **Note**: his output is usually not a good summary of the semantic content of the input, you’re often better with averaging or pooling the sequence of hidden-states for the whole input sequence.\n",
    "\n",
    "- **last_hidden_state**: Sequence of hidden-states at the output of the last layer of the model, averaged over the tokens.\n",
    "\n",
    "- **hidden_state**: Hidden states of the model at the output of each layer, averaged over the tokens.\n",
    "\n",
    "- **hidden_state_cls**: See *hidden_state* but use the CLS token output.\n",
    "\n",
    "If *hidden_state* or *hidden_state_cls* is used as embedding type, you also need to pass the layer numbers used to extract the embedding from. As an example we extract embeddings from the last 8 hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/avl/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_tf_utils:loading weights file https://cdn.huggingface.co/bert-base-cased-tf_model.h5 from cache at /home/avl/.cache/torch/transformers/17e64dc7dc200314bc70dd8198010773501bcabb65a493c1ae7183b8c9a5b1ff.908e74db1113031d6827eb22808cf370b0aeded6e6ac20d0f07af0a334e195cc.h5\n",
      "INFO:transformers.modeling_tf_utils:Layers from pretrained model not used in TFBertModel: ['nsp___cls', 'mlm___cls']\n"
     ]
    }
   ],
   "source": [
    "emb_type = 'hidden_state'\n",
    "n_layers = 8\n",
    "layers = [-_ for _ in range(1, n_layers + 1)]\n",
    "\n",
    "Embedding = TransformerEmbedding(model_name, emb_type, layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what an embedding looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 768)\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.batch_encode_plus(X[:5], pad_to_max_length=True, \n",
    "                                     max_length=max_len, return_tensors='tf')\n",
    "emb = Embedding(tokens)\n",
    "print(emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the BERT model's embedding space used by the drift detector consists of a $768$-dimensional vector for each instance. We will therefore first apply a dimensionality reduction step with an Untrained AutoEncoder (*UAE*) before conducting the statistical hypothesis test. We use the embedding model as the input for the UAE which then projects the embedding on a lower dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dim = 32\n",
    "shape = (emb.shape[1],)\n",
    "\n",
    "uae = UAE(input_layer=Embedding, shape=shape, enc_dim=enc_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 32)\n"
     ]
    }
   ],
   "source": [
    "emb_uae = uae(tokens)\n",
    "print(emb_uae.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-S detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize\n",
    "\n",
    "We proceed to initialize the drift detector. From here on the detector works the same as for other modalities such as images. Please check the [images](https://docs.seldon.io/projects/alibi-detect/en/latest/examples/cd_ks_cifar10.html) example or the [K-S detector documentation](https://docs.seldon.io/projects/alibi-detect/en/latest/methods/ksdrift.html) for more information about each of the possible parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define preprocessing step parameters\n",
    "preprocess_kwargs = {\n",
    "    'model': uae,\n",
    "    'tokenizer': tokenizer,\n",
    "    'max_len': max_len,\n",
    "    'batch_size': 32\n",
    "}\n",
    "\n",
    "cd = KSDrift(\n",
    "    p_val=.05,\n",
    "    X_ref=X_ref,            # reference data to test against\n",
    "    preprocess_X_ref=True,  # store preprocessed X_ref for future predict calls\n",
    "    preprocess_kwargs=preprocess_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reference data is already preprocessed and stored to save time at each predict call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert cd.X_ref.shape[1] == enc_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving and loading detectors is straightforward. In order to keep the preprocessing flexibility we cannot make strong assumptions and need to pass the optional `preprocess_fn` and/or `preprocess_kwargs` to the loading function. This allows to have preprocessing steps defined in other frameworks such as PyTorch or scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:alibi_detect.utils.saving:Directory my_path does not exist and is now created.\n"
     ]
    }
   ],
   "source": [
    "filepath = 'my_path'\n",
    "save_detector(cd, filepath)\n",
    "\n",
    "kwargs = {'preprocess_kwargs': preprocess_kwargs}\n",
    "cd = load_detector(filepath, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect drift\n",
    "\n",
    "Let's first check if drift occurs on a similar sample from the training set as the reference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drift? No!\n",
      "p-value: [0.79439443 0.93558097 0.60991895 0.82795686 0.7590978  0.5360543\n",
      " 0.79439443 0.99365413 0.50035924 0.93558097 0.722555   0.26338065\n",
      " 0.06155144 0.14833806 0.5726548  0.99365413 0.96887016 0.64755726\n",
      " 0.6852314  0.40047103 0.03778438 0.50035924 0.46576622 0.9882611\n",
      " 0.85929435 0.06155144 0.64755726 0.28769323 0.12050407 0.5360543\n",
      " 0.50035924 0.05464633]\n"
     ]
    }
   ],
   "source": [
    "preds_h0 = cd.predict(X_h0, return_p_val=True)\n",
    "labels = ['No!', 'Yes!']\n",
    "print('Drift? {}'.format(labels[preds_h0['data']['is_drift']]))\n",
    "print('p-value: {}'.format(preds_h0['data']['p_val']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect drift on imbalanced and perturbed datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% negative sentiment 10.0\n",
      "Drift? Yes!\n",
      "p-value: [1.99518353e-01 3.40991944e-01 7.94394433e-01 5.00359237e-01\n",
      " 5.00359237e-01 5.72654784e-01 3.40991944e-01 2.92505771e-02\n",
      " 6.91903234e-02 9.99870896e-01 4.00471032e-01 9.13475513e-01\n",
      " 8.69054198e-02 1.08282514e-01 4.00471032e-01 7.59097815e-01\n",
      " 4.65766221e-01 4.28151786e-02 1.08282514e-01 9.68870163e-01\n",
      " 8.87938619e-01 2.87693232e-01 8.59294355e-01 3.13561678e-01\n",
      " 9.80161786e-01 3.32311448e-03 9.68870163e-01 7.26078229e-04\n",
      " 3.40991944e-01 9.69783217e-03 6.09918952e-01 4.00471032e-01]\n",
      "\n",
      "% negative sentiment 90.0\n",
      "Drift? Yes!\n",
      "p-value: [2.0958899e-01 1.6769221e-01 4.6035737e-01 8.2974398e-01 5.1026817e-02\n",
      " 3.6845976e-04 1.3959650e-02 3.4691233e-03 8.4126943e-01 2.1410163e-02\n",
      " 3.8667390e-01 2.3008208e-01 2.5486696e-01 3.5239363e-01 5.4717654e-01\n",
      " 2.0238158e-01 9.8914808e-01 3.1717645e-04 1.3743564e-01 7.9330452e-02\n",
      " 8.4309745e-01 1.8943239e-02 9.8286802e-01 6.8996549e-01 9.8215282e-01\n",
      " 1.5109187e-01 8.3644027e-01 3.2119218e-01 1.3976933e-01 4.5221341e-01\n",
      " 1.5445707e-02 4.0925053e-01]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in X_imb.items():\n",
    "    preds = cd.predict(v, return_p_val=True)\n",
    "    print('% negative sentiment {}'.format(k * 100))\n",
    "    print('Drift? {}'.format(labels[preds['data']['is_drift']]))\n",
    "    print('p-value: {}'.format(preds['data']['p_val']))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: fantastic -- % perturbed: 1.0\n",
      "Drift? Yes!\n",
      "p-value: [8.59294355e-01 4.84188050e-02 2.87693232e-01 8.87938619e-01\n",
      " 2.87693232e-01 3.77843790e-02 2.56591532e-02 5.72654784e-01\n",
      " 3.40991944e-01 9.98707950e-01 7.59097815e-01 4.00471032e-01\n",
      " 3.50604125e-04 8.87938619e-01 9.99870896e-01 9.99543309e-01\n",
      " 8.87938619e-01 7.94394433e-01 8.69054198e-02 8.27956855e-01\n",
      " 9.13475513e-01 3.69972497e-01 3.77843790e-02 6.85231388e-01\n",
      " 2.63380647e-01 1.33834302e-01 7.22554982e-01 9.54058170e-01\n",
      " 1.20504074e-01 2.56591532e-02 9.88261104e-01 8.27956855e-01]\n",
      "\n",
      "Word: fantastic -- % perturbed: 5.0\n",
      "Drift? Yes!\n",
      "p-value: [1.0324168e-03 2.7371673e-15 1.0030026e-10 8.2795686e-01 5.4185481e-08\n",
      " 1.1215132e-18 2.5042721e-10 1.4893160e-02 1.5452230e-12 2.4072561e-04\n",
      " 3.3278044e-02 1.0030026e-10 0.0000000e+00 8.6666952e-04 8.2795686e-01\n",
      " 1.2274054e-03 3.6997250e-01 2.9250577e-02 3.2487294e-19 4.2815179e-02\n",
      " 1.6396579e-04 5.7140245e-15 6.0547863e-20 2.2463709e-02 1.8834284e-17\n",
      " 1.1058156e-11 8.6905420e-02 5.0035924e-01 1.2831078e-31 1.0453582e-26\n",
      " 5.6953936e-06 6.0707825e-04]\n",
      "\n",
      "Word: good -- % perturbed: 1.0\n",
      "Drift? Yes!\n",
      "p-value: [1.9951835e-01 9.9365413e-01 4.2815179e-02 9.9954331e-01 1.9951835e-01\n",
      " 9.9693102e-01 4.6576622e-01 9.3558097e-01 9.3558097e-01 6.8523139e-01\n",
      " 9.5405817e-01 8.5929435e-01 2.2463709e-02 9.9987090e-01 9.9365413e-01\n",
      " 4.6576622e-01 9.9999607e-01 5.7265478e-01 9.9954331e-01 8.6905420e-02\n",
      " 9.1347551e-01 8.5929435e-01 2.8769323e-01 8.5929435e-01 8.5929435e-01\n",
      " 9.9693102e-01 9.9693102e-01 9.9987090e-01 7.9439443e-01 1.6396579e-04\n",
      " 9.1347551e-01 9.6887016e-01]\n",
      "\n",
      "Word: good -- % perturbed: 5.0\n",
      "Drift? Yes!\n",
      "p-value: [6.1319246e-16 5.7265478e-01 9.2428386e-20 9.8826110e-01 6.1410643e-10\n",
      " 3.6997250e-01 3.2578668e-05 4.0047103e-01 1.2274054e-03 5.4185481e-08\n",
      " 7.2255498e-01 2.4034434e-03 1.8678642e-37 7.2131259e-03 1.3383430e-01\n",
      " 3.3837957e-10 3.4099194e-01 8.2482254e-10 9.3558097e-01 1.4201371e-13\n",
      " 9.5405817e-01 1.7943768e-06 1.9783097e-07 1.6396579e-04 8.6666952e-04\n",
      " 6.1551444e-02 4.2815179e-02 3.6997250e-01 6.1497598e-09 0.0000000e+00\n",
      " 5.3222836e-03 5.0035924e-01]\n",
      "\n",
      "Word: bad -- % perturbed: 1.0\n",
      "Drift? No!\n",
      "p-value: [0.64755726 0.6852314  0.2406036  0.96887016 0.50035924 0.82795686\n",
      " 0.9999727  0.82795686 0.96887016 0.5360543  0.96887016 0.6852314\n",
      " 0.00203786 0.9882611  0.96887016 0.5726548  0.26338065 0.09710453\n",
      " 0.7590978  0.34099194 0.1640792  0.6852314  0.12050407 0.79439443\n",
      " 0.99870795 0.99870795 0.99999607 0.9540582  0.6852314  0.18111965\n",
      " 0.12050407 0.99999607]\n",
      "\n",
      "Word: bad -- % perturbed: 5.0\n",
      "Drift? Yes!\n",
      "p-value: [3.18301190e-08 3.49877549e-09 4.19758215e-16 5.32228360e-03\n",
      " 1.71956726e-05 1.99518353e-01 6.47557259e-01 1.33834302e-01\n",
      " 1.98871276e-04 2.00923371e-13 4.00471032e-01 3.27475419e-07\n",
      " 0.00000000e+00 2.26972293e-06 3.13561678e-01 2.92505771e-02\n",
      " 2.63813617e-05 8.49670826e-18 1.64079204e-01 7.04859247e-08\n",
      " 1.29587926e-24 1.07232365e-08 4.14164800e-17 2.24637091e-02\n",
      " 2.87693232e-01 6.09918952e-01 1.48338065e-01 9.69783217e-03\n",
      " 1.00300261e-10 3.24872937e-19 1.88342838e-17 5.72654784e-01]\n",
      "\n",
      "Word: horrible -- % perturbed: 1.0\n",
      "Drift? Yes!\n",
      "p-value: [0.28769323 0.9540582  0.9998709  0.93558097 0.64755726 0.9540582\n",
      " 0.9540582  0.99365413 0.01293455 0.99999607 0.99870795 0.5360543\n",
      " 0.06155144 0.9998709  0.8879386  0.9540582  0.02925058 0.46576622\n",
      " 0.93558097 0.3699725  0.28769323 0.96887016 0.26338065 0.996931\n",
      " 0.9998709  0.02925058 0.9134755  0.996931   0.996931   0.00145631\n",
      " 0.2406036  0.79439443]\n",
      "\n",
      "Word: horrible -- % perturbed: 5.0\n",
      "Drift? Yes!\n",
      "p-value: [9.0348902e-17 3.6997250e-01 5.0035924e-01 9.1347551e-01 1.2274054e-03\n",
      " 1.1211077e-02 6.1551444e-02 2.8289410e-03 1.1508834e-28 7.2607823e-04\n",
      " 2.8769323e-01 3.3837957e-10 4.3195025e-41 3.3278044e-02 7.2607823e-04\n",
      " 3.6997250e-01 4.6432427e-09 2.4072561e-04 6.1410643e-10 7.0521917e-14\n",
      " 8.7644167e-30 1.1119027e-05 3.9587313e-15 7.2255498e-01 3.6098195e-06\n",
      " 1.3007273e-15 8.5929435e-01 1.2274054e-03 7.0521917e-14 0.0000000e+00\n",
      " 1.8887657e-15 4.8418805e-02]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for w, probas in X_word.items():\n",
    "    for p, v in probas.items():\n",
    "        preds = cd.predict(v, return_p_val=True)\n",
    "        print('Word: {} -- % perturbed: {}'.format(w, p))\n",
    "        print('Drift? {}'.format(labels[preds['data']['is_drift']]))\n",
    "        print('p-value: {}'.format(preds['data']['p_val']))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMD detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize\n",
    "\n",
    "Again check the [images](https://docs.seldon.io/projects/alibi-detect/en/latest/examples/cd_mmd_cifar10.html) example or the [MMD detector documentation](https://docs.seldon.io/projects/alibi-detect/en/latest/methods/ksdrift.html) for more information about each of the possible parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = MMDDrift(\n",
    "    p_val=.05,\n",
    "    X_ref=X_ref,            # reference data to test against\n",
    "    preprocess_X_ref=True,  # store preprocessed X_ref for future predict calls\n",
    "    preprocess_kwargs=preprocess_kwargs,\n",
    "    chunk_size=1000,\n",
    "    n_permutations=10       # nb of permutations in the test, set to 10 for runtime\n",
    ")                           # purposes; should be much higher for a real test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect drift\n",
    "\n",
    "*H0*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drift? No!\n",
      "p-value: 1.0\n"
     ]
    }
   ],
   "source": [
    "preds_h0 = cd.predict(X_h0, return_p_val=True)\n",
    "labels = ['No!', 'Yes!']\n",
    "print('Drift? {}'.format(labels[preds_h0['data']['is_drift']]))\n",
    "print('p-value: {}'.format(preds_h0['data']['p_val']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% negative sentiment 10.0\n",
      "Drift? Yes!\n",
      "p-value: 0.0\n",
      "\n",
      "% negative sentiment 90.0\n",
      "Drift? Yes!\n",
      "p-value: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in X_imb.items():\n",
    "    preds = cd.predict(v, return_p_val=True)\n",
    "    print('% negative sentiment {}'.format(k * 100))\n",
    "    print('Drift? {}'.format(labels[preds['data']['is_drift']]))\n",
    "    print('p-value: {}'.format(preds['data']['p_val']))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perturbed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: fantastic -- % perturbed: 1.0\n",
      "Drift? No!\n",
      "p-value: 0.3\n",
      "\n",
      "Word: fantastic -- % perturbed: 5.0\n",
      "Drift? Yes!\n",
      "p-value: 0.0\n",
      "\n",
      "Word: good -- % perturbed: 1.0\n",
      "Drift? No!\n",
      "p-value: 1.0\n",
      "\n",
      "Word: good -- % perturbed: 5.0\n",
      "Drift? Yes!\n",
      "p-value: 0.0\n",
      "\n",
      "Word: bad -- % perturbed: 1.0\n",
      "Drift? No!\n",
      "p-value: 1.0\n",
      "\n",
      "Word: bad -- % perturbed: 5.0\n",
      "Drift? Yes!\n",
      "p-value: 0.0\n",
      "\n",
      "Word: horrible -- % perturbed: 1.0\n",
      "Drift? No!\n",
      "p-value: 0.6\n",
      "\n",
      "Word: horrible -- % perturbed: 5.0\n",
      "Drift? Yes!\n",
      "p-value: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for w, probas in X_word.items():\n",
    "    for p, v in probas.items():\n",
    "        preds = cd.predict(v, return_p_val=True)\n",
    "        print('Word: {} -- % perturbed: {}'.format(w, p))\n",
    "        print('Drift? {}'.format(labels[preds['data']['is_drift']]))\n",
    "        print('p-value: {}'.format(preds['data']['p_val']))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *MMD* detector is less sensitive than the *K-S* drift detector for the perturbed sentences.\n",
    "\n",
    "## Train embeddings from scratch\n",
    "\n",
    "So far we used pre-trained embeddings from a BERT model. We can however also use embeddings from a model trained from scratch. First we define and train a simple classification model consisting of an embedding and LSTM layer.\n",
    "\n",
    "### Load data and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb, reuters\n",
    "from tensorflow.keras.layers import Dense, Embedding, Input, LSTM\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "INDEX_FROM = 3\n",
    "NUM_WORDS = 10000\n",
    "\n",
    "\n",
    "def print_sentence(tokenized_sentence: str, id2w: dict):\n",
    "    print(' '.join(id2w[_] for _ in tokenized_sentence))\n",
    "    print('')\n",
    "    print(tokenized_sentence)\n",
    "\n",
    "\n",
    "def mapping_word_id(data):\n",
    "    w2id = data.get_word_index()\n",
    "    w2id = {k: (v + INDEX_FROM) for k, v in w2id.items()}\n",
    "    w2id[\"<PAD>\"] = 0\n",
    "    w2id[\"<START>\"] = 1\n",
    "    w2id[\"<UNK>\"] = 2\n",
    "    w2id[\"<UNUSED>\"] = 3\n",
    "    id2w = {v: k for k, v in w2id.items()}\n",
    "    return w2id, id2w\n",
    "\n",
    "\n",
    "def get_dataset(dataset: str = 'imdb', max_len: int = 100):\n",
    "    if dataset == 'imdb':\n",
    "        data = imdb\n",
    "    elif dataset == 'reuters':\n",
    "        data = reuters\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    w2id, id2w = mapping_word_id(data)\n",
    "\n",
    "    (X_train, y_train), (X_test, y_test) = data.load_data(\n",
    "        num_words=NUM_WORDS, index_from=INDEX_FROM)\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "    y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test), (w2id, id2w)\n",
    "\n",
    "\n",
    "def imdb_model(X: np.ndarray, num_words: int = 100, emb_dim: int = 128,\n",
    "               lstm_dim: int = 128, output_dim: int = 2) -> tf.keras.Model:\n",
    "    inputs = Input(shape=(X.shape[1:]), dtype=tf.float32)\n",
    "    x = Embedding(num_words, emb_dim)(inputs)\n",
    "    x = LSTM(lstm_dim, dropout=.5)(x)\n",
    "    outputs = Dense(output_dim, activation=tf.nn.softmax)(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and tokenize data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test), (word2token, token2word) = \\\n",
    "    get_dataset(dataset='imdb', max_len=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out an instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
      "\n",
      "[1415   33    6   22   12  215   28   77   52    5   14  407   16   82\n",
      "    2    8    4  107  117 5952   15  256    4    2    7 3766    5  723\n",
      "   36   71   43  530  476   26  400  317   46    7    4    2 1029   13\n",
      "  104   88    4  381   15  297   98   32 2071   56   26  141    6  194\n",
      " 7486   18    4  226   22   21  134  476   26  480    5  144   30 5535\n",
      "   18   51   36   28  224   92   25  104    4  226   65   16   38 1334\n",
      "   88   12   16  283    5   16 4472  113  103   32   15   16 5345   19\n",
      "  178   32]\n"
     ]
    }
   ],
   "source": [
    "print_sentence(X_train[0], token2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and train a simple model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 123s 5ms/sample - loss: 0.4250 - accuracy: 0.8016 - val_loss: 0.3550 - val_accuracy: 0.8460\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 123s 5ms/sample - loss: 0.2759 - accuracy: 0.8900 - val_loss: 0.3782 - val_accuracy: 0.8367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff1a8749d90>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = imdb_model(X=X_train, num_words=NUM_WORDS, emb_dim=256, lstm_dim=128, output_dim=2)\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=2, \n",
    "          shuffle=True, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the embedding layer from the trained model and combine with UAE preprocessing step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 100, 256)\n"
     ]
    }
   ],
   "source": [
    "Embedding = tf.keras.Model(inputs=model.inputs, outputs=model.layers[1].output)\n",
    "emb = Embedding(X_train[:5])\n",
    "print(emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "shape = tuple(emb.shape[1:])\n",
    "uae = UAE(input_layer=Embedding, shape=shape, enc_dim=enc_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, create reference, *H0* and perturbed datasets. Also test against the *Reuters* news topic classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ref, y_ref = random_sample(X_test, y_test, proba_zero=.5, n=n_sample)\n",
    "X_h0, y_h0 = random_sample(X_test, y_test, proba_zero=.5, n=n_sample)\n",
    "tokens = [word2token[w] for w in words]\n",
    "X_word = {}\n",
    "for i, t in enumerate(tokens):\n",
    "    X_word[words[i]] = {}\n",
    "    for p in perc_chg:\n",
    "        X_word[words[i]][p] = inject_word(t, X_ref, p, padding='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load and tokenize Reuters dataset\n",
    "(X_reut, y_reut), (w2t_reut, t2w_reut) = \\\n",
    "    get_dataset(dataset='reuters', max_len=max_len)[1:]\n",
    "\n",
    "# sample random instances\n",
    "idx = np.random.choice(X_reut.shape[0], n_sample, replace=False)\n",
    "X_ood = X_reut[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize detector and detect drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need for a tokenizer since we are already working with tokenized instances\n",
    "preprocess_kwargs = {'model': uae, 'batch_size': 128}\n",
    "\n",
    "cd = KSDrift(\n",
    "    p_val=.05,\n",
    "    X_ref=X_ref,\n",
    "    preprocess_X_ref=True,\n",
    "    preprocess_kwargs=preprocess_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*H0*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drift? No!\n",
      "p-value: [0.64755726 0.43243074 0.8879386  0.2406036  0.9540582  0.9134755\n",
      " 0.85929435 0.9134755  0.08690542 0.96887016 0.722555   0.9134755\n",
      " 0.9134755  0.8879386  0.85929435 0.40047103 0.18111965 0.19951835\n",
      " 0.1338343  0.50035924 0.9540582  0.9134755  0.28769323 0.5360543\n",
      " 0.28769323 0.34099194 0.85929435 0.04841881 0.21933001 0.6852314\n",
      " 0.3699725  0.5360543 ]\n"
     ]
    }
   ],
   "source": [
    "preds_h0 = cd.predict(X_h0, return_p_val=True)\n",
    "labels = ['No!', 'Yes!']\n",
    "print('Drift? {}'.format(labels[preds_h0['data']['is_drift']]))\n",
    "print('p-value: {}'.format(preds_h0['data']['p_val']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perturbed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: fantastic -- % perturbed: 1.0\n",
      "Drift? No!\n",
      "p-value: [0.9998709  0.9882611  0.9134755  0.8879386  0.9540582  0.9540582\n",
      " 0.60991895 0.93558097 0.28769323 0.99870795 0.82795686 0.82795686\n",
      " 0.5360543  0.9540582  0.6852314  0.99870795 0.9134755  0.99870795\n",
      " 0.82795686 0.9134755  0.9801618  0.996931   0.82795686 0.7590978\n",
      " 0.99870795 0.96887016 0.9801618  0.9882611  0.5726548  0.9995433\n",
      " 0.8879386  0.99365413]\n",
      "\n",
      "Word: fantastic -- % perturbed: 5.0\n",
      "Drift? Yes!\n",
      "p-value: [1.2274054e-03 3.6997250e-01 2.8769323e-01 7.5909781e-01 2.6338065e-01\n",
      " 1.3383430e-01 9.0799862e-05 7.7621467e-02 2.2697229e-06 8.5929435e-01\n",
      " 5.3605431e-01 7.9439443e-01 7.7621467e-02 4.6576622e-01 4.3243074e-01\n",
      " 7.9439443e-01 3.4099194e-01 8.5929435e-01 8.6905420e-02 3.3278044e-02\n",
      " 1.4833806e-01 8.2795686e-01 4.6576622e-01 2.8769323e-01 1.3383430e-01\n",
      " 6.0991895e-01 3.1356168e-01 1.6407920e-01 8.8793862e-01 1.9626908e-02\n",
      " 7.7621467e-02 2.4060360e-01]\n",
      "\n",
      "Word: good -- % perturbed: 1.0\n",
      "Drift? No!\n",
      "p-value: [0.9995433  0.9882611  0.9134755  0.9995433  0.99870795 0.99365413\n",
      " 0.85929435 0.99870795 0.9882611  0.9999727  0.9995433  0.99870795\n",
      " 0.9995433  0.9540582  0.99365413 0.82795686 0.9998709  0.93558097\n",
      " 0.9998709  0.99870795 0.9540582  0.96887016 0.79439443 0.996931\n",
      " 0.7590978  0.93558097 0.9882611  0.996931   0.96887016 0.96887016\n",
      " 0.99365413 0.93558097]\n",
      "\n",
      "Word: good -- % perturbed: 5.0\n",
      "Drift? No!\n",
      "p-value: [0.82795686 0.996931   0.34099194 0.60991895 0.9801618  0.9134755\n",
      " 0.10828251 0.9882611  0.9882611  0.31356168 0.28769323 0.46576622\n",
      " 0.9134755  0.93558097 0.46576622 0.50035924 0.3699725  0.9801618\n",
      " 0.722555   0.60991895 0.7590978  0.9540582  0.722555   0.93558097\n",
      " 0.21933001 0.9134755  0.43243074 0.85929435 0.5726548  0.722555\n",
      " 0.6852314  0.07762147]\n",
      "\n",
      "Word: bad -- % perturbed: 1.0\n",
      "Drift? No!\n",
      "p-value: [0.9540582  0.64755726 0.9801618  0.99870795 0.9134755  0.9540582\n",
      " 0.9801618  0.82795686 0.8879386  0.7590978  0.99365413 0.99870795\n",
      " 0.8879386  0.8879386  0.93558097 0.9540582  0.7590978  0.64755726\n",
      " 0.82795686 0.96887016 0.99999607 0.9995433  0.96887016 0.722555\n",
      " 0.722555   0.93558097 0.6852314  0.93558097 0.79439443 0.8879386\n",
      " 0.9134755  0.9882611 ]\n",
      "\n",
      "Word: bad -- % perturbed: 5.0\n",
      "Drift? Yes!\n",
      "p-value: [7.21312594e-03 1.12110768e-02 1.20504074e-01 4.00471032e-01\n",
      " 5.00359237e-01 2.92505771e-02 3.13561678e-01 1.96269080e-02\n",
      " 1.08282514e-01 3.32780443e-02 1.71140861e-02 8.59294355e-01\n",
      " 8.37208051e-03 9.35580969e-01 7.59097815e-01 1.48931602e-02\n",
      " 7.94394433e-01 8.69054198e-02 8.69054198e-02 9.13475513e-01\n",
      " 5.00359237e-01 7.22554982e-01 7.59097815e-01 3.50604125e-04\n",
      " 1.20504074e-01 1.48338065e-01 3.77843790e-02 1.64079204e-01\n",
      " 2.40603596e-01 4.28151786e-02 2.40603596e-01 1.99518353e-01]\n",
      "\n",
      "Word: horrible -- % perturbed: 1.0\n",
      "Drift? No!\n",
      "p-value: [0.99365413 0.85929435 0.9134755  0.85929435 0.79439443 0.7590978\n",
      " 0.79439443 0.31356168 0.64755726 0.46576622 0.9540582  0.7590978\n",
      " 0.43243074 0.9134755  0.9540582  0.60991895 0.8879386  0.64755726\n",
      " 0.85929435 0.9882611  0.93558097 0.93558097 0.99365413 0.79439443\n",
      " 0.9134755  0.43243074 0.64755726 0.9134755  0.64755726 0.93558097\n",
      " 0.93558097 0.9801618 ]\n",
      "\n",
      "Word: horrible -- % perturbed: 5.0\n",
      "Drift? Yes!\n",
      "p-value: [1.71140861e-02 5.32228360e-03 4.28151786e-02 2.92505771e-02\n",
      " 2.92505771e-02 1.79437677e-06 4.55808453e-03 1.72444014e-03\n",
      " 6.07078255e-04 1.45630504e-03 1.72444014e-03 2.24637091e-02\n",
      " 8.00581650e-12 6.09918952e-01 1.08282514e-01 4.93855441e-05\n",
      " 1.99518353e-01 1.10792353e-04 7.21312594e-03 4.00471032e-01\n",
      " 4.00471032e-01 4.65766221e-01 3.40991944e-01 2.24637091e-02\n",
      " 7.26078229e-04 4.55808453e-03 8.76041099e-07 2.40344345e-03\n",
      " 1.07232365e-08 7.76214674e-02 1.99518353e-01 4.00471032e-01]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for w, probas in X_word.items():\n",
    "    for p, v in probas.items():\n",
    "        preds = cd.predict(v, return_p_val=True)\n",
    "        print('Word: {} -- % perturbed: {}'.format(w, p))\n",
    "        print('Drift? {}'.format(labels[preds['data']['is_drift']]))\n",
    "        print('p-value: {}'.format(preds['data']['p_val']))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the detector is not as sensitive as the Transformer-based K-S drift detector. The embeddings trained from scratch only trained on a small dataset and a simple model with cross-entropy loss function for 2 epochs. The pre-trained BERT model on the other hand captures semantics of the data better.\n",
    "\n",
    "Sample from the Reuters dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drift? Yes!\n",
      "p-value: [1.34916729e-04 2.83701773e-13 2.53623026e-18 4.28151786e-02\n",
      " 2.63380647e-01 6.20218972e-03 1.22740539e-03 3.77843790e-02\n",
      " 4.64324268e-09 1.08282514e-01 1.84965307e-10 3.32780443e-02\n",
      " 7.13247118e-06 3.24872937e-19 1.64079204e-01 1.00300261e-10\n",
      " 8.91430864e-06 1.45630504e-03 1.79437677e-06 5.46463318e-02\n",
      " 6.09918952e-01 1.47906931e-09 8.12879719e-09 1.34916729e-04\n",
      " 7.76214674e-02 5.46463318e-02 1.47906931e-09 1.71956726e-05\n",
      " 1.47906931e-09 2.13202584e-05 7.13247118e-06 1.22740539e-03]\n"
     ]
    }
   ],
   "source": [
    "preds_ood = cd.predict(X_ood, return_p_val=True)\n",
    "labels = ['No!', 'Yes!']\n",
    "print('Drift? {}'.format(labels[preds_ood['data']['is_drift']]))\n",
    "print('p-value: {}'.format(preds_ood['data']['p_val']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:detect] *",
   "language": "python",
   "name": "conda-env-detect-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
