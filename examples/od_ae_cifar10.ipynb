{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE outlier detection on CIFAR10\n",
    "\n",
    "## Dataset\n",
    "\n",
    "[CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) consists of 60,000 32 by 32 RGB images equally distributed over 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, \\\n",
    "    Dense, Layer, Reshape, InputLayer, Flatten\n",
    "from tqdm import tqdm\n",
    "\n",
    "from alibi_detect.models.losses import elbo\n",
    "from alibi_detect.od import OutlierAE\n",
    "from alibi_detect.utils.perturbation import apply_mask\n",
    "from alibi_detect.utils.saving import save_detector, load_detector\n",
    "from alibi_detect.utils.visualize import plot_instance_score, plot_feature_outlier_image\n",
    "\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = tf.keras.datasets.cifar10.load_data()\n",
    "X_train, y_train = train\n",
    "X_test, y_test = test\n",
    "\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load or define outlier detector\n",
    "\n",
    "The pretrained outlier and adversarial detectors used in the example notebooks can be found [here](https://console.cloud.google.com/storage/browser/seldon-models/alibi-detect). You can either manually download the relevant files in the [od_vae_cifar10](https://console.cloud.google.com/storage/browser/seldon-models/alibi-detect/od_vae_cifar10/) folder to e.g. the local directory ```my_dir```. Alternatively,  if you have [Google Cloud SDK](https://cloud.google.com/sdk/docs/) installed, you can download the whole folder as follows:\n",
    "\n",
    "```bash\n",
    "!gsutil cp -r gs://seldon-models/alibi-detect/od_vae_cifar10 my_dir\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gsutil cp -r gs://seldon-models/alibi-detect/od_vae_cifar10 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_outlier_detector = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepath = './od_ae_cifar10/'  # change to directory where model is downloaded\n",
    "if load_outlier_detector:  # load pretrained outlier detector\n",
    "    od = load_detector(filepath)\n",
    "else:  # define model, initialize, train and save outlier detector\n",
    "    \n",
    "    encoder_net = tf.keras.Sequential(\n",
    "      [\n",
    "          InputLayer(input_shape=(32, 32, 3)),\n",
    "          Conv2D(64, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
    "          Conv2D(128, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
    "          Conv2D(512, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
    "          Flatten(),\n",
    "          Dense(1024,)\n",
    "      ])\n",
    "\n",
    "    decoder_net = tf.keras.Sequential(\n",
    "      [\n",
    "          InputLayer(input_shape=(1024,)),\n",
    "          Dense(4*4*128),\n",
    "          Reshape(target_shape=(4, 4, 128)),\n",
    "          Conv2DTranspose(256, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
    "          Conv2DTranspose(64, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
    "          Conv2DTranspose(3, 4, strides=2, padding='same', activation='sigmoid')\n",
    "      ])\n",
    "    \n",
    "    # initialize outlier detector\n",
    "    od = OutlierAE(threshold=.015,  # threshold for outlier score\n",
    "                    encoder_net=encoder_net,  # can also pass VAE model instead\n",
    "                    decoder_net=decoder_net,  # of separate encoder and decoder\n",
    "                    )\n",
    "    # train\n",
    "    od.fit(X_train,\n",
    "           epochs=50\n",
    "           verbose=True)\n",
    "    \n",
    "    # save the trained outlier detector\n",
    "    save_detector(od, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check quality VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 8\n",
    "X = X_train[idx].reshape(1, 32, 32, 3)\n",
    "X_recon = od.vae(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X.reshape(32, 32, 3))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_recon.numpy().reshape(32, 32, 3))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check outliers on original CIFAR images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train[:500]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_preds = od.predict(X,\n",
    "                      outlier_type='instance',    # use 'feature' or 'instance' level\n",
    "                      return_feature_score=True,  # scores used to determine outliers\n",
    "                      return_instance_score=True)\n",
    "print(list(od_preds['data'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_preds['data']['instance_score'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot instance level outlier scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.zeros(X.shape[0],).astype(int)  # all normal CIFAR10 training instances\n",
    "labels = ['normal', 'outlier']\n",
    "plot_instance_score(od_preds, target, labels, od.threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_recon = od.vae(X).numpy()\n",
    "plot_feature_outlier_image(od_preds, \n",
    "                           X, \n",
    "                           X_recon=X_recon,\n",
    "                           instance_ids=[8, 60, 100, 330],  # pass a list with indices of instances to display\n",
    "                           max_instances=5,  # max nb of instances to display\n",
    "                           outliers_only=False)  # only show outlier predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict outliers on perturbed CIFAR images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perturb CIFAR images by adding random noise to patches (masks) of the image. For each mask size in `n_mask_sizes`, sample `n_masks` and apply those to each of the `n_imgs` images. Then we predict outliers on the masked instances: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb of predictions per image: n_masks * n_mask_sizes \n",
    "n_mask_sizes = 10\n",
    "n_masks = 20\n",
    "n_imgs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define masks and get images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_sizes = [(2*n,2*n) for n in range(1,n_mask_sizes+1)]\n",
    "print(mask_sizes)\n",
    "img_ids = np.arange(n_imgs)\n",
    "X_orig = X[img_ids].reshape(img_ids.shape[0], 32, 32, 3)\n",
    "print(X_orig.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate instance level outlier scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_img_scores = []\n",
    "for i in tqdm(range(X_orig.shape[0])):\n",
    "    img_scores = np.zeros((len(mask_sizes),))\n",
    "    for j, mask_size in enumerate(mask_sizes):\n",
    "        # create masked instances\n",
    "        X_mask, mask = apply_mask(X_orig[i].reshape(1, 32, 32, 3),\n",
    "                                  mask_size=mask_size,\n",
    "                                  n_masks=n_masks,\n",
    "                                  channels=[0,1,2],\n",
    "                                  mask_type='normal',\n",
    "                                  noise_distr=(0,1),\n",
    "                                  clip_rng=(0,1))\n",
    "        # predict outliers\n",
    "        od_preds_mask = od.predict(X_mask)\n",
    "        score = od_preds_mask['data']['instance_score']\n",
    "        # store average score over `n_masks` for a given mask size\n",
    "        img_scores[j] = np.mean(score)\n",
    "    all_img_scores.append(img_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize outlier scores vs. mask sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plt = [mask[0] for mask in mask_sizes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ais in all_img_scores:\n",
    "    plt.plot(x_plt, ais)\n",
    "    plt.xticks(x_plt)\n",
    "plt.title('Outlier Score All Images for Increasing Mask Size')\n",
    "plt.xlabel('Mask size')\n",
    "plt.ylabel('Outlier Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ais_np = np.zeros((len(all_img_scores), all_img_scores[0].shape[0]))\n",
    "for i, ais in enumerate(all_img_scores):\n",
    "    ais_np[i, :] = ais\n",
    "ais_mean = np.mean(ais_np, axis=0)\n",
    "plt.title('Mean Outlier Score All Images for Increasing Mask Size')\n",
    "plt.xlabel('Mask size')\n",
    "plt.ylabel('Outlier score')\n",
    "plt.plot(x_plt, ais_mean)\n",
    "plt.xticks(x_plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate instance level outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 8  # index of instance to look at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_plt, all_img_scores[i])\n",
    "plt.xticks(x_plt)\n",
    "plt.title('Outlier Scores Image {} for Increasing Mask Size'.format(i))\n",
    "plt.xlabel('Mask size')\n",
    "plt.ylabel('Outlier score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruction of masked images and outlier scores per channel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_X_mask = []\n",
    "X_i = X_orig[i].reshape(1, 32, 32, 3)\n",
    "all_X_mask.append(X_i)\n",
    "# apply masks\n",
    "for j, mask_size in enumerate(mask_sizes):\n",
    "    # create masked instances\n",
    "    X_mask, mask = apply_mask(X_i,\n",
    "                              mask_size=mask_size,\n",
    "                              n_masks=1,  # just 1 for visualization purposes\n",
    "                              channels=[0,1,2],\n",
    "                              mask_type='normal',\n",
    "                              noise_distr=(0,1),\n",
    "                              clip_rng=(0,1))\n",
    "    all_X_mask.append(X_mask)\n",
    "all_X_mask = np.concatenate(all_X_mask, axis=0)\n",
    "all_X_recon = od.vae(all_X_mask).numpy()\n",
    "od_preds = od.predict(all_X_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_outlier_image(od_preds, \n",
    "                           all_X_mask, \n",
    "                           X_recon=all_X_recon, \n",
    "                           max_instances=all_X_mask.shape[0], \n",
    "                           n_channels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict outliers on a subset of features\n",
    "\n",
    "The sensitivity of the outlier detector can not only be controlled via the `threshold`, but also by selecting the percentage of the features used for the instance level outlier score computation. For instance, we might want to flag outliers if 40% of the features (pixels for images) have an average outlier score above the threshold. This is possible via the `outlier_perc` argument in the `predict` function. It specifies the percentage of the features that are used for outlier detection, sorted in descending outlier score order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_list = [20, 40, 60, 80, 100]\n",
    "\n",
    "all_perc_scores = []\n",
    "for perc in perc_list:\n",
    "    od_preds_perc = od.predict(all_X_mask, outlier_perc=perc)\n",
    "    iscore = od_preds_perc['data']['instance_score']\n",
    "    all_perc_scores.append(iscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize outlier scores vs. mask sizes and percentage of features used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_perc_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plt = [0] + x_plt\n",
    "for aps in all_perc_scores:\n",
    "    plt.plot(x_plt, aps)\n",
    "    plt.xticks(x_plt)\n",
    "plt.legend(perc_list)\n",
    "plt.title('Outlier Score for Increasing Mask Size and Different Feature Subsets')\n",
    "plt.xlabel('Mask Size')\n",
    "plt.ylabel('Outlier Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer outlier threshold value\n",
    "\n",
    "Finding good threshold values can be tricky since they are typically not easy to interpret. The `infer_threshold` method helps finding a sensible value. We need to pass a batch of instances `X` and specify what percentage of those we consider to be normal via `threshold_perc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Current threshold: {}'.format(od.threshold))\n",
    "od.infer_threshold(X, threshold_perc=99)  # assume 1% of the training data are outliers\n",
    "print('New threshold: {}'.format(od.threshold))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
